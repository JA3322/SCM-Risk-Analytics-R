#Outlier Treatment for Income
x <- bank_dataset$Income..in.K.month.
qnt <- quantile(x, probs = c(.25, .75), na.rm = T)
qnt
caps <- quantile(x, probs = c(.05, .95), na.rm = T)
caps
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] <- caps[1]
x[x > (qnt[2] + H)] <- caps[2]
bank_dataset$Income..in.K.month. <- x
boxplot(bank_dataset$Income..in.K.month., main="Box plot of Annual Income of Customer", horizontal = TRUE, col = "blue")
#Outlier Treatment for CCAvg
x <- bank_dataset$CCAvg
qnt <- quantile(x, probs = c(.25, .75), na.rm = T)
qnt
caps <- quantile(x, probs = c(.10, .90), na.rm = T)
caps
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] <- caps[1]
x[x > (qnt[2] + H)] <- caps[2]
bank_dataset$CCAvg <- x
boxplot(bank_dataset$CCAvg, main="Box plot of Avg Credit Card spending", horizontal = TRUE, col = "blue")
#Outlier Treatment for Mortgage
x <- bank_dataset$Mortgage
qnt <- quantile(x, probs = c(.25, .75), na.rm = T)
qnt
caps <- quantile(x, probs = c(.10, .90), na.rm = T)
caps
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] <- caps[1]
x[x > (qnt[2] + H)] <- caps[2]
bank_dataset$Mortgage <- x
boxplot(bank_dataset$Mortgage, main="Box plot of Customer House Mortgage", horizontal = TRUE, col = "blue")
smoted_bank_dataset <- SMOTE(CreditCard~., bank_dataset,perc.over = 200, perc.under = 150)
bank_index <-sample(2,nrow(bank_dataset), replace=TRUE, prob = c(0.75,0.25))
bank_train <- bank_dataset[bank_index==1,]
bank_test <- bank_dataset[bank_index==2,]
bank_lda <- lda(CreditCard~., smoted_bank_dataset)
View(smoted_bank_dataset)
colnames(bank_train)
# Prediction on Training Dataset
bank_prediction <- predict(bank_lda, bank_train[,-12])
#Histogram for LD1
ldahist(data = bank_prediction$x[,1], g=bank_test$CreditCard)
#Histogram for LD1
ldahist(data = bank_prediction$x[,1], g=bank_test$CreditCard)
#Confusion Matrix and Accuracy on Training Data
train_predict <- predict(bank_lda, bank_train)$class
train_matrix <- table(Predicted = train_predict, Actual = bank_train$CreditCard)
train_matrix
sum(diag(train_matrix))/sum(train_matrix) # accuracy
#Confusion Matrix and Accuracy on Test Data
test_predict <- predict(bank_lda, bank_test)$class
test_matrix <- table(Predicted = test_predict, Actual = bank_test$CreditCard)
test_matrix
sum(diag(test_matrix))/sum(test_matrix)
bank_index <-sample(2,nrow(smoted_bank_dataset), replace=TRUE, prob = c(0.75,0.25))
bank_train <- smoted_bank_dataset[smoted_bank_dataset==1,]
bank_test <- smoted_bank_dataset[smoted_bank_dataset==2,]
bank_lda <- lda(CreditCard~., smoted_bank_dataset)
# Prediction on Training Dataset
bank_prediction <- predict(bank_lda, bank_train[,-12])
bank_prediction
View(bank_dataset)
bank_index <-sample(2,nrow(smoted_bank_dataset), replace=TRUE, prob = c(0.75,0.25))
bank_train <- smoted_bank_dataset[bank_index==1,]
bank_test <- smoted_bank_dataset[bank_index==2,]
bank_lda <- lda(CreditCard~., smoted_bank_dataset)
# Prediction on Training Dataset
bank_prediction <- predict(bank_lda, bank_train[,-12])
#Histogram for LD1
ldahist(data = bank_prediction$x[,1], g=bank_test$CreditCard)
#Confusion Matrix and Accuracy on Training Data
train_predict <- predict(bank_lda, bank_train)$class
train_matrix <- table(Predicted = train_predict, Actual = bank_train$CreditCard)
train_matrix
sum(diag(train_matrix))/sum(train_matrix) # accuracy
#Confusion Matrix and Accuracy on Test Data
test_predict <- predict(bank_lda, bank_test)$class
test_matrix <- table(Predicted = test_predict, Actual = bank_test$CreditCard)
test_matrix
sum(diag(test_matrix))/sum(test_matrix)
bank_index <-sample(2,nrow(bank_dataset), replace=TRUE, prob = c(0.75,0.25))
bank_train <- bank_dataset[bank_index==1,]
bank_test <- bank_dataset[bank_index==2,]
bank_lda <- lda(CreditCard~., bank_dataset)
bank_lda
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 55), tidy = TRUE)
cat('Instances which correspond to no Credit card ', nrow(bank_dataset[bank_dataset$CreditCard==0,]))
cat('Instances which correspond to having a Credit card ', nrow(bank_dataset[bank_dataset$CreditCard==1,]))
cat('Instances which correspond to no Credit card ', nrow(bank_dataset[bank_dataset$CreditCard==0,]), '\n')
cat('Instances which correspond to having a Credit card ', nrow(bank_dataset[bank_dataset$CreditCard==1,]), '\n')
smoted_bank_dataset <- SMOTE(CreditCard~., bank_dataset,perc.over = 200, perc.under = 150)
nrow(smoted_bank_dataset[smoted_bank_dataset$CreditCard == 1,])
nrow(smoted_bank_dataset[smoted_bank_dataset$CreditCard == 0,])
painter <- c(x = '10', y ='20')
class(painter) <- 'Screeen_Painter'
findX.Screeen_Painter <- function(object) {
cat('\n X is', object$x, '\n')
}
findX(painter)
studentBio <- list(studentName = "Harry Potter", studentAge = 19, studentContact="London")
class(studentBio) <- "StudentInfo"
studentBio
contact.StudentInfo <- function(object) {
cat("Your contact is", object$studentContact, "\n")
}
contact(studentBio)
contact <- function(object) {
UseMethod("contact")
}
contact(studentBio)
jpeg("CorrelationAnalysis.jpg", width = 350, height = "350")
jpeg("CorrelationAnalysis.jpg", width = 350, height = 350)
data <- read.csv(file.choose(), header = T)
library("forecast")
library("ggplot2")
library("tseries")
#Starting year is given as 1980 and starts from month 1 (January)
#Since yearly data frequency is given as 12
cola = ts(data, start=c(1980,1), frequency = 12)
##Converting the data to univariate series
cola = ts(as.numeric(cola), start=c(1980,1), frequency = 12)
## Simple Plot
reg_cola <- lm(cola ~ time(cola))
plot(cola, main = "Cola Sales")
abline(reg_cola, col = "blue")
#Decompose data to look at the time series components
decomp <- stl(cola, s.window = 3)
#Deseasonalize the data
#seasadj returns the seasonally adjusted data constructed by removing the seasonal component
deseasoned_cola <- seasadj(decomp)
colaTrain <- window(deseasoned_cola, start=1980, end = c(2009,12))
colaTest <- window(deseasoned_cola, start = 2010)
#Decompose data to look at the time series components
decomp <- stl(cola, s.window = 3)
data <- read.csv(file.choose(), header = T)
#Starting year is given as 1980 and starts from month 1 (January)
#Since yearly data frequency is given as 12
cola = ts(data, start=c(1980,1), frequency = 12)
##Converting the data to univariate series
cola = ts(as.numeric(cola), start=c(1980,1), frequency = 12)
## Simple Plot
reg_cola <- lm(cola ~ time(cola))
#Decompose data to look at the time series components
decomp <- stl(cola, s.window = 3)
plot(cola, main = "Cola Sales")
#Month Plot
monthplot(cola)
cola
#cex - character expansion
seasonplot(cola,xlab = "",
col = c("red", "blue", "green"),
year.labels = T, labelgap = 0.50,
cex = 1.0,
main = "Seasonal Plot of Cola Sales")
#Decompose data to look at the time series components
decomp <- stl(cola, s.window = 3)
data <- read.csv(file.choose(), header = T)
data <- read.csv(file.choose(), header = T)
#Starting year is given as 1980 and starts from month 1 (January)
#Since yearly data frequency is given as 12
cola = ts(data, start=c(1980,1), frequency = 12)
##Converting the data to univariate series
cola = ts(as.numeric(cola), start=c(1980,1), frequency = 12)
## Simple Plot
reg_cola <- lm(cola ~ time(cola))
#Decompose data to look at the time series components
decomp <- stl(cola, s.window = 3)
#Deseasonalize the data
#seasadj returns the seasonally adjusted data constructed by removing the seasonal component
deseasoned_cola <- seasadj(decomp)
colaTrain <- window(deseasoned_cola, start=1980, end = c(2009,12))
colaTest <- window(deseasoned_cola, start = 2010)
ses_cola<-ses(colaTrain,h=12)
autoplot(ses_cola)
summary(ses_cola)
holt_cola<-holt(colaTrain, h=12)
autoplot(holt_cola)
summary(holt_cola)
accuracy(holt_cola, colaTest)
accuracy(ses_cola,colaTest)
##Holt Winters - Additive Model
cola_HWA <- ets(colaTrain, model="AAA")
autoplot(forecast(cola_HWA))
summary(cola_HWA)
#Forecast for next 12 months and accuracy
cola_HWA_forecast <- forecast(cola_HWA, h=12)
accuracy(cola_HWA_forecast, colaTest)
##Holt Winters - Multiplicative Model
cola_HWM <- ets(colaTrain, model="MAM")
autoplot(forecast(cola_HWM))
summary(cola_HWM)
#Forecast for next 12 months
cola_HWM_forecast <- forecast(cola_HWM, h=12)
accuracy(cola_HWM_forecast, colaTest)
## Check for stationarity
adf.test(deseasoned_cola, alternative = "stationary")
## Differencing the time series data - to remove the trend
detrended_cola = diff(deseasoned_cola, differences=1)
plot(detrended_cola)
## Check for stationarity
adf.test(deseasoned_cola, alternative = "stationary")
## Differencing the time series data - to remove the trend
detrended_cola = diff(deseasoned_cola, differences=1)
plot(detrended_cola)
#Augmented Dickey Fuller Test
adf.test(detrended_cola, alternative = "stationary")
#Deseasonalising for second order
detrended_cola = diff(deseasoned_cola, differences=2)
plot(detrended_cola)
#ACF plot on differenced time series data
acf(detrended_cola, main = "ACF for differenced series")
#PACF plot on differenced time series data
pacf(detrended_cola, main = "PACF for differenced series")
##Build the ARIMA model with (p,d,q) = (1,2,1)
# (p,d,q) : PACF->p; ACF->q; diff->d
colaARIMA1 = arima(colaTrain, order = c(1, 2, 1))
colaARIMA1
##Residual Analysis - Box Ljung Test
library(stats)
Box.test(colaARIMA2$residuals, type = c("Ljung-Box"))
##Forecasting with the ARIMA model
fcast_colaArima = forecast(colaARIMA2, h=12)
fcast_colaArima
# Plotting the forecast
plot(fcast_colaArima)
## Accuracy with ARIMA model
accuracy(fcast_colaArima, colaTest)
#Residual Analysis
res<-residuals(colaARIMA1)
gghistogram(res) + ggtitle("Histogram of residuals")
##Residual Analysis - Box Ljung Test
library(stats)
Box.test(colaARIMA1$residuals, type = c("Ljung-Box"))
# Retry with p=2, d=2, q=1
colaARIMA2 = arima(colaTrain, order = c(2, 2, 1))
colaARIMA2
#Residual Analysis
res<-residuals(colaARIMA2)
gghistogram(res) + ggtitle("Histogram of residuals")
##Residual Analysis - Box Ljung Test
library(stats)
Box.test(colaARIMA2$residuals, type = c("Ljung-Box"))
##Forecasting with the ARIMA model
fcast_colaArima = forecast(colaARIMA2, h=12)
fcast_colaArima
# Plotting the forecast
plot(fcast_colaArima)
## Accuracy with ARIMA model
accuracy(fcast_colaArima, colaTest)
##Fitting with Auto ARIMA
AArima <- auto.arima(colaTrain, seasonal = FALSE)
AArima
##Residual Analysis - Box Ljung Test for Auto ARIMA
resAArima<-residuals(AArima)
gghistogram(resAArima) + ggtitle("Histogram of residuals")
Box.test(AArima$residuals, type = c("Ljung-Box"))
##Forcasting with auto ARIMA model
fcast_autoArima = forecast(AArima, h=12)
fcast_autoArima
#Plotting Auto ARIMA model
plot(fcast_autoArima)
## Accuracy with auto ARIMA model
accuracy(fcast_autoArima, colaTest)
library(gbm)
install.packages("gbm")
install.packages("ipred")
iibrary(ROCR)
install.packages("ROCR")
install.packages("dgof")
install.packages("ineq")
install.packages("DMwR")
# Loading the dataset
churn <- read.csv(file.choose(), header =TRUE)
# Structure of the dataset
str(churn)
# Treatment of Outliers
x <- churn$calls
quant<-quantile(x,probs = c(.25,.75),na.rm = TRUE)
quant
replace<-quantile(x,probs = c(.05,.95),na.rm = TRUE)
replace
Outlier <-1.5 * IQR(x,na.rm=TRUE)
x[x < (quant[1]-Outlier)]<-replace[1]
x[x > (quant[2]+ Outlier)]<-replace[2]
churn$calls <-x
boxplot(churn$calls, main = "Boxplot of number of calls in a month", xlab="Number of calls in a
month", col="blue",horizontal = TRUE)
x <- churn$charges
quant<-quantile(x,probs = c(.25,.75),na.rm = TRUE)
quant
replace<-quantile(x,probs = c(.05,.95),na.rm = TRUE)
replace
Outlier <-1.5 * IQR(x,na.rm=TRUE)
x[x < (quant[1]-Outlier)]<-replace[1]
x[x > (quant[2]+ Outlier)]<-replace[2]
churn$charges <-x
boxplot(churn$charges, main = "Boxplot of charges amount in a month", xlab="Charges in a
month",col="pink",horizontal = TRUE)
x <- churn$coverage
quant<-quantile(x,probs = c(.25,.75),na.rm = TRUE)
quant
replace<-quantile(x,probs = c(.05,.95),na.rm = TRUE)
replace
Outlier <-1.5 * IQR(x,na.rm=TRUE)
x[x < (quant[1]-Outlier)]<-replace[1]
x[x > (quant[2]+ Outlier)]<-replace[2]
churn$coverage <-x
boxplot(churn$coverage, main = "Boxplot of number of days in a month without coverage",
xlab="Number of days in a month without coverage",col="brown",horizontal = TRUE)
library(DataExplorer)
plot_missing(churn)
meancalls
# Replacing the na values of continous variables with mean:Mean imputation
# Calculating the mean
meancalls<- mean(churn$calls[!is.na(churn$calls)])
meancalls
# Replacing the na values in calls column with the mean
churn$calls[is.na(churn$calls)]<-meancalls
# Replacing the na values of categorical variables with mode:Mode imputation
# Calculating the mode
mode <-function(x){
uniq <- unique(x)
uniq[which.max(tabulate(match(x,uniq)))]
}
modeage<- mode(churn$age)
modeage
# Replacing the na values in age column with the mode
churn$age[is.na(churn$age)]<-modeage
View(churn)
# Removing phoneno and zipcode column
churn<- churn[-1]
churn<- churn[-3]
# Structure of the dataset
str(churn)
# Converting the variable 'churn' into categorical variable
churn$churn <- as.factor(churn$churn)
library(caret)
# Splitting data into train and test
set.seed(1234)
s <- sample(c(1:5000), size = 4000)
churn.train <- churn[s,]
churn.test <- churn[-s,]
library(rpart)
library(gbm)
?gbm
str(churn)
?glm
?gbm
#Boosting
# n.trees= number of trees
# interaction.depth = depth of the tree or
#the number of splits in the tree (default value is 1)
#shrinkage =learning rate which helps reduce errors
#cv.folds = cross validation folds
#n.cores = CPU cores
#verbose = false indicates that the progress of the
#model will not be printed when in process.
{
time1 <- Sys.time()
gbm.fit <- gbm(
formula = churn.train$churn ~ .,
distribution = "bernoulli",
data = churn.train,
n.trees = 500,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
n.cores = 2,
verbose = TRUE,
)
time2<-Sys.time()
cat("\nTrain GBM took ", difftime(time1, time2, units = "mins"))
}
str(churn)
churn$age <- as.factor(churn$age)
churn$sim <- as.factor(churn$sim)
churn$phone <- as.factor(churn$phone)
churn$prepost <- as.factor(churn$prepost)
# Loading the dataset
churn <- read.csv(file.choose(), header =TRUE)
# Treatment of Outliers
x <- churn$calls
quant<-quantile(x,probs = c(.25,.75),na.rm = TRUE)
quant
replace<-quantile(x,probs = c(.05,.95),na.rm = TRUE)
replace
Outlier <-1.5 * IQR(x,na.rm=TRUE)
x[x < (quant[1]-Outlier)]<-replace[1]
x[x > (quant[2]+ Outlier)]<-replace[2]
churn$calls <-x
boxplot(churn$calls, main = "Boxplot of number of calls in a month", xlab="Number of calls in a
month", col="blue",horizontal = TRUE)
x <- churn$charges
quant<-quantile(x,probs = c(.25,.75),na.rm = TRUE)
quant
replace<-quantile(x,probs = c(.05,.95),na.rm = TRUE)
replace
Outlier <-1.5 * IQR(x,na.rm=TRUE)
x[x < (quant[1]-Outlier)]<-replace[1]
x[x > (quant[2]+ Outlier)]<-replace[2]
churn$charges <-x
boxplot(churn$charges, main = "Boxplot of charges amount in a month", xlab="Charges in a
month",col="pink",horizontal = TRUE)
x <- churn$coverage
quant<-quantile(x,probs = c(.25,.75),na.rm = TRUE)
quant
replace<-quantile(x,probs = c(.05,.95),na.rm = TRUE)
replace
Outlier <-1.5 * IQR(x,na.rm=TRUE)
x[x < (quant[1]-Outlier)]<-replace[1]
x[x > (quant[2]+ Outlier)]<-replace[2]
churn$coverage <-x
boxplot(churn$coverage, main = "Boxplot of number of days in a month without coverage",
xlab="Number of days in a month without coverage",col="brown",horizontal = TRUE)
# Dealing with na values
#install.packages("DataExplorer")
library(DataExplorer)
plot_missing(churn)
# Replacing the na values of continous variables with mean:Mean imputation
# Calculating the mean
meancalls<- mean(churn$calls[!is.na(churn$calls)])
meancalls
# Replacing the na values in calls column with the mean
churn$calls[is.na(churn$calls)]<-meancalls
# Replacing the na values of categorical variables with mode:Mode imputation
# Calculating the mode
mode <-function(x){
uniq <- unique(x)
uniq[which.max(tabulate(match(x,uniq)))]
}
modeage<- mode(churn$age)
modeage
# Replacing the na values in age column with the mode
churn$age[is.na(churn$age)]<-modeage
View(churn)
# Removing phoneno and zipcode column
churn<- churn[-1]
churn<- churn[-3]
# Structure of the dataset
str(churn)
# Converting the variable 'churn' into categorical variable
churn$churn <- as.factor(churn$churn)
install.packages("caret")
#install.packages("caret")
library(caret)
# Splitting data into train and test
set.seed(1234)
s <- sample(c(1:5000), size = 4000)
churn.train <- churn[s,]
churn.test <- churn[-s,]
# Install the necessary packages
#install.packages("rpart")
#install.packages("gbm")
library(rpart)
churn$gender <- as.factor(churn$gender)
churn$sim <- as.factor(churn$sim)
churn$phone <- as.factor(churn$phone)
churn$prepost <- as.factor(churn$prepost)
str(churn)
{
time1 <- Sys.time()
gbm.fit <- gbm(
formula = churn.train$churn ~ .,
distribution = "bernoulli",
data = churn.train,
n.trees = 500,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
n.cores = 2,
verbose = TRUE,
)
time2<-Sys.time()
cat("\nTrain GBM took ", difftime(time1, time2, units = "mins"))
}
str(churn)
s <- sample(c(1:5000), size = 4000)
churn.train <- churn[s,]
churn.test <- churn[-s,]
str(churn.train)
{
time1 <- Sys.time()
gbm.fit <- gbm(
formula = churn.train$churn ~ .,
distribution = "bernoulli",
data = churn.train,
n.trees = 500,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
n.cores = 2,
verbose = TRUE,
)
time2<-Sys.time()
cat("\nTrain GBM took ", difftime(time1, time2, units = "mins"))
}
if (rstudioapi::isAvailable()) {
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
source("Utils.R")
source("Package_Handling.R")
#Significant Columns for Logistic Regression
significant_columns <- c("Type", "Days.for.shipping..real.", "Days.for.shipment..scheduled.", "Longitude", "Order.Status", "Shipping.Mode")
lr_sig_scm_data <- scm_data[, which(colnames(scm_data) %in% significant_columns)]
#To skip Data Preprocessing and read directly preprocessed data
scm_data <- read.csv("./Data/Data_processed.csv", header = TRUE)
lr_sig_scm_data <- scm_data[, which(colnames(scm_data) %in% significant_columns)]
names(lr_sig_scm_data)
